train.py \
--stage sft \
--do_train True \
--model_name_or_path /root/.cache/modelscope/hub/Qwen2-0.5B \
--preprocessing_num_workers 16 \
--finetuning_type full \
--template qwen \
--flash_attn auto \
--dataset_dir /app/data \
--dataset alpaca_zh \
--cutoff_len 1024 \
--learning_rate 5e-05 \
--num_train_epochs 3.0 \
--max_samples 1000 \
--per_device_train_batch_size 2 \
--gradient_accumulation_steps 8 \
--lr_scheduler_type cosine \
--max_grad_norm 1.0 \
--logging_steps 5 \
--save_steps 100 \
--warmup_steps 0 \
--optim adamw_torch \
--packing False \
--report_to none \
--use_badam True \
--output_dir /app/saves/Qwen2-0.5B/full/train_test \
--fp16 True \
--plot_loss True \
--ddp_timeout 180000000 \
--include_num_input_tokens_seen True \
--badam_mode layer \
--badam_switch_mode ascending \
--badam_switch_interval 50 \
--badam_update_ratio 0.05 \
--val_size 0.1 \
--eval_strategy steps \
--eval_steps 100 \
--per_device_eval_batch_size 2 